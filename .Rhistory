col="darkgrey",
ylab = "Final Exam Scores",
xlab = "Midterm Scores",
main= "Relationship Between Midterm and Final Exam Scores"
)
set.seed(42) #Allows us to always get the same "random" sample
#Change to a new number to get a new sample
n <- 30 #set the sample size
X_i <- runif(n, -5, 55)
#Gives n random values from a uniform distribution between 15 to 45.
X_2i <- sample(c(0,1,2), n, replace=TRUE)
#Gives n 0's and 1's randomly
beta0 <- 3 #Our choice for the y-intercept.
beta1 <- 2.5 #Our choice for the slope.
beta2 <- -20 #negative 17 is the new y intercept
beta3 <- 3.5 #change in the slope added to orignal when x2 turns on
sigma <- 3 #Our choice for the std. deviation of the error terms.
epsilon_i <- rnorm(n, 0, sigma)
#Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.
Y_i <- beta0 + beta1*X_i + beta2*X_2i + beta3*X_i*X_2i + epsilon_i
#Create Y using the normal error regression model
fabData <- data.frame(y=Y_i, x=X_i, x2=X_2i)
#Store the data as data
#In the real world, we begin with data (like fabData) and try to recover the model that
# (we assume) was used to created it.
fab.lm <- lm(y ~ x + x2 + x:x2, data=fabData) #Fit an estimated regression model to the fabData.
#summary(fab.lm) %>% pander() #Summarize your model.
plot(y ~ x, data=fabData, col=as.factor(x2)) #Plot the data.
#abline(fab.lm) #Add the estimated regression line to your plot.
b <- coef(fab.lm)
x2 = 0
curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE)
x2 = 1
curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE, col="red")
x2 = 2
curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE, col="green")
# Now for something you can't do in real life... but since we created the data...
# abline(beta0, beta1, lty=2)
#Add the true regression line to your plot using a dashed line (lty=2).
x2=0
curve(beta0 + beta1*x + beta2*x2 +beta3*x*x2, add=TRUE, lty=2)
x2=1
curve(beta0 + beta1*x + beta2*x2 +beta3*x*x2, add=TRUE, lty=2, col="red")
legend("topleft", legend=c("True Line", "Estimated Line"), lty=c(2,1), bty="n")
#Add a legend to your plot specifying which line is which.
pander(fab.lm)
library(pander)
set.seed(42) #Allows us to always get the same "random" sample
#Change to a new number to get a new sample
n <- 30 #set the sample size
X_i <- runif(n, -5, 55)
#Gives n random values from a uniform distribution between 15 to 45.
X_2i <- sample(c(0,1,2), n, replace=TRUE)
#Gives n 0's and 1's randomly
beta0 <- 3 #Our choice for the y-intercept.
beta1 <- 2.5 #Our choice for the slope.
beta2 <- -20 #negative 17 is the new y intercept
beta3 <- 3.5 #change in the slope added to orignal when x2 turns on
sigma <- 3 #Our choice for the std. deviation of the error terms.
epsilon_i <- rnorm(n, 0, sigma)
#Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.
Y_i <- beta0 + beta1*X_i + beta2*X_2i + beta3*X_i*X_2i + epsilon_i
#Create Y using the normal error regression model
fabData <- data.frame(y=Y_i, x=X_i, x2=X_2i)
#Store the data as data
#In the real world, we begin with data (like fabData) and try to recover the model that
# (we assume) was used to created it.
fab.lm <- lm(y ~ x + x2 + x:x2, data=fabData) #Fit an estimated regression model to the fabData.
#summary(fab.lm) %>% pander() #Summarize your model.
plot(y ~ x, data=fabData, col=as.factor(x2)) #Plot the data.
#abline(fab.lm) #Add the estimated regression line to your plot.
b <- coef(fab.lm)
x2 = 0
curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE)
x2 = 1
curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE, col="red")
x2 = 2
curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE, col="green")
# Now for something you can't do in real life... but since we created the data...
# abline(beta0, beta1, lty=2)
#Add the true regression line to your plot using a dashed line (lty=2).
x2=0
curve(beta0 + beta1*x + beta2*x2 +beta3*x*x2, add=TRUE, lty=2)
x2=1
curve(beta0 + beta1*x + beta2*x2 +beta3*x*x2, add=TRUE, lty=2, col="red")
legend("topleft", legend=c("True Line", "Estimated Line"), lty=c(2,1), bty="n")
#Add a legend to your plot specifying which line is which.
pander(fab.lm)
pander(fabData)
set.seed(42) #Allows us to always get the same "random" sample
#Change to a new number to get a new sample
n <- 30 #set the sample size
X_i <- runif(n, -5, 55)
#Gives n random values from a uniform distribution between 15 to 45.
X_2i <- sample(c(0,1,2), n, replace=TRUE)
#Gives n 0's and 1's randomly
beta0 <- 3 #Our choice for the y-intercept.
beta1 <- 2.5 #Our choice for the slope.
beta2 <- -20 #negative 17 is the new y intercept
beta3 <- 3.5 #change in the slope added to orignal when x2 turns on
sigma <- 3 #Our choice for the std. deviation of the error terms.
epsilon_i <- rnorm(n, 0, sigma)
#Gives n random values from a normal distribution with mean = 0, st. dev. = sigma.
Y_i <- beta0 + beta1*X_i + beta2*X_2i + beta3*X_i*X_2i + epsilon_i
#Create Y using the normal error regression model
fabData <- data.frame(y=Y_i, x=X_i, x2=X_2i)
#Store the data as data
#In the real world, we begin with data (like fabData) and try to recover the model that
# (we assume) was used to created it.
fab.lm <- lm(y ~ x + x2 + x:x2, data=fabData) #Fit an estimated regression model to the fabData.
#summary(fab.lm) %>% pander() #Summarize your model.
plot(y ~ x, data=fabData, col=as.factor(x2)) #Plot the data.
#abline(fab.lm) #Add the estimated regression line to your plot.
b <- coef(fab.lm)
x2 = 0
curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE)
x2 = 1
curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE, col="red")
x2 = 2
curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE, col="green")
# Now for something you can't do in real life... but since we created the data...
# abline(beta0, beta1, lty=2)
#Add the true regression line to your plot using a dashed line (lty=2).
x2=0
curve(beta0 + beta1*x + beta2*x2 +beta3*x*x2, add=TRUE, lty=2)
x2=1
curve(beta0 + beta1*x + beta2*x2 +beta3*x*x2, add=TRUE, lty=2, col="red")
legend("topleft", legend=c("True Line", "Estimated Line"), lty=c(2,1), bty="n")
#Add a legend to your plot specifying which line is which.
pander(fab.lm)
pander(fabData)
plot(y ~ x, data=fabData, col=as.factor(x2)) #Plot the data.
#abline(fab.lm) #Add the estimated regression line to your plot.
b <- coef(fab.lm)
x2 = 0
curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE)
x2 = 1
curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE, col="red")
x2 = 2
curve(b[1] + b[2]*x + b[3]*x2 + b[4]*x*x2, add=TRUE, col="green")
train <- read.csv("C:\Users\Jared\OneDrive\Desktop\Math 425\house-prices-advanced-regression-techniques-1\train.csv", stringsAsFactors = TRUE)
train <- read.csv("C:/Users/Jared/OneDrive/Desktop/Math 425/house-prices-advanced-regression-techniques-1/train.csv", stringsAsFactors = TRUE)
train <- train %>%
mutate(Alley = as.character(Alley),
Alley = replace_na(Alley, "None"),
Alley = as.factor(Alley)) %>%
mutate(TotalSF = X1stFlrSF + X2ndFlrSF + TotalBsmtSF,
RichNbrhd = case_when(Neighborhood %in% c("StoneBr", "NridgHt", "NoRidge") ~ 1,
TRUE ~ 0))
library(tidyverse)
train <- read.csv("C:/Users/Jared/OneDrive/Desktop/Math 425/house-prices-advanced-regression-techniques-1/train.csv", stringsAsFactors = TRUE)
train <- train %>%
mutate(Alley = as.character(Alley),
Alley = replace_na(Alley, "None"),
Alley = as.factor(Alley)) %>%
mutate(TotalSF = X1stFlrSF + X2ndFlrSF + TotalBsmtSF,
RichNbrhd = case_when(Neighborhood %in% c("StoneBr", "NridgHt", "NoRidge") ~ 1,
TRUE ~ 0))
plot(SalePrice ~ Neighborhood, data=train, las=2)
lm1 <- lm(SalePrice ~ Neighborhood, data=train)
summary(lm1)
lm.1stflr <- lm(SalePrice ~ X1stFlrSF, data=train)
summary(lm.1stflr)
plot(SalePrice ~ X1stFlrSF, data=train)
lm.alley <- lm(SalePrice ~ Alley, data=train)
summary(lm.alley)
plot(SalePrice ~ Alley, data=train)
table(train$Alley)
View(train[,c("SalePrice","Alley")])
lm.fence <- lm(SalePrice ~ Fence, data=train)
summary(lm.fence)
apply(train, 2, function(x) sum(is.na(x))) #count missing values
lm.1stflr <- lm(SalePrice ~ X1stFlrSF, data=train)
summary(lm.1stflr)
lm.2ndflr <- lm(SalePrice ~ X2ndFlrSF, data=train)
summary(lm.2ndflr)
lm.basement <- lm(SalePrice ~ TotalBsmtSF, data=train)
summary(lm.basement)
lm.sqft.all <- lm(SalePrice ~ X1stFlrSF + X2ndFlrSF + TotalBsmtSF, data=train)
summary(lm.sqft.all)
train <- train %>%
mutate(TotalSF = X1stFlrSF + X2ndFlrSF + TotalBsmtSF)
lm.sqft <- lm(SalePrice ~ TotalSF, data=train)
summary(lm.sqft)
plot(SalePrice ~ TotalSF, data=train)
lm.sqft.rich <- lm(SalePrice ~ TotalSF + RichNbrhd + TotalSF:RichNbrhd, data=train)
summary(lm.sqft.rich)
lm.sqft.rich.log <- lm(log(SalePrice) ~ TotalSF + RichNbrhd + TotalSF:RichNbrhd, data=train)
summary(lm.sqft.rich)
exp(coef(lm.sqft.rich.log))
exp(coef(lm.sqft.rich.log)[2]*1000)
exp(coef(lm.sqft.rich.log)[2]*1000)
exp(coef(lm.sqft.rich.log)[2]*1000)
```
plot(SalesPrice~ TotalSF + RichNbrhd + TotalSF:RichNbrhd, data=train)
plot(SalePrice~ TotalSF + RichNbrhd + TotalSF:RichNbrhd, data=train)
library(tidyverse)
train <- read.csv("C:/Users/Jared/OneDrive/Desktop/Math 425/house-prices-advanced-regression-techniques-1/train.csv", stringsAsFactors = TRUE)
train <- train %>%
mutate(Alley = as.character(Alley),
Alley = replace_na(Alley, "None"),
Alley = as.factor(Alley)) %>%
mutate(TotalSF = X1stFlrSF + X2ndFlrSF + TotalBsmtSF,
RichNbrhd = case_when(Neighborhood %in% c("StoneBr", "NridgHt", "NoRidge") ~ 1,
TRUE ~ 0))
plot(SalePrice ~ Neighborhood, data=train, las=2)
lm1 <- lm(SalePrice ~ Neighborhood, data=train)
summary(lm1)
lm.1stflr <- lm(SalePrice ~ X1stFlrSF, data=train)
summary(lm.1stflr)
plot(SalePrice ~ X1stFlrSF, data=train)
lm.alley <- lm(SalePrice ~ Alley, data=train)
summary(lm.alley)
plot(SalePrice ~ Alley, data=train)
table(train$Alley)
View(train[,c("SalePrice","Alley")])
lm.fence <- lm(SalePrice ~ Fence, data=train)
summary(lm.fence)
apply(train, 2, function(x) sum(is.na(x))) #count missing values
lm.1stflr <- lm(SalePrice ~ X1stFlrSF, data=train)
summary(lm.1stflr)
lm.2ndflr <- lm(SalePrice ~ X2ndFlrSF, data=train)
summary(lm.2ndflr)
lm.basement <- lm(SalePrice ~ TotalBsmtSF, data=train)
summary(lm.basement)
lm.sqft.all <- lm(SalePrice ~ X1stFlrSF + X2ndFlrSF + TotalBsmtSF, data=train)
summary(lm.sqft.all)
train <- train %>%
mutate(TotalSF = X1stFlrSF + X2ndFlrSF + TotalBsmtSF)
lm.sqft <- lm(SalePrice ~ TotalSF, data=train)
summary(lm.sqft)
plot(SalePrice ~ TotalSF, data=train)
lm.sqft.rich <- lm(SalePrice ~ TotalSF + RichNbrhd + TotalSF:RichNbrhd, data=train)
summary(lm.sqft.rich)
lm.sqft.rich.log <- lm(log(SalePrice) ~ TotalSF + RichNbrhd + TotalSF:RichNbrhd, data=train)
summary(lm.sqft.rich)
exp(coef(lm.sqft.rich.log))
exp(coef(lm.sqft.rich.log)[2]*1000)
exp(coef(lm.sqft.rich.log)[2]*1000)
```{r}
house3d <- lm(SalePrice ~ TotalSF + LotArea + TotalSF:LotArea, data=train)
summary(house3d)
## To embed the 3d-scatterplot inside of your html document is harder.
library(plotly)
library(reshape2)
#Setup Axis
axis_x <- seq(min(train$TotalSF), max(train$TotalSF), length.out=100)
axis_y <- seq(min(train$LotArea), max(train$LotArea), length.out=100)
#Sample points
air_surface <- expand.grid(TotalSF = axis_x, LotArea = axis_y, KEEP.OUT.ATTRS=F)
air_surface$Z <- predict.lm(house3d, newdata = air_surface)
air_surface <- acast(air_surface, LotArea ~ TotalSF, value.var = "Z") #y ~ x
#Create scatterplot
plot_ly(train,
x = ~TotalSF,
y = ~LotArea,
z = ~SalePrice,
type = "scatter3d",
mode = "markers") %>%
add_trace(z = air_surface,
x = axis_x,
y = axis_y,
type = "surface")
plot(SalePrice ~ TotalSF, data=train, ylim=c(-1000000,1000000))
b <- coef(house3d)
b
drawit <- function(LotArea, col=col){
curve(b[1] + b[2]*TotalSF + b[3]*LotArea + b[4]*TotalSF*LotArea, add=TRUE, col=col, xname="TotalSF")
}
for (la in axis_y){
drawit(la, col="red")
}
drawit(1300, col="red")
drawit(215245, col="blue")
househd <- lm(SalePrice ~ TotalSF + LotArea + GarageArea + Alley + FullBath + ScreenPorch, data=train)
summary(househd)
```{r}
set.seed(121)
num_rows <- 1000 #1460 total
keep <- sample(1:nrow(train), num_rows)
mytrain <- train[keep, ] #Use this in the lm(..., data=mytrain) it is like "rbdata"
mytest <- train[-keep, ] #Use this in the predict(..., newdata=mytest) it is like "rbdata2"
househd <- lm(SalePrice ~ TotalSF + LotArea + GarageArea + Alley + FullBath + ScreenPorch, data=mytrain)
summary(househd)
house3d <- lm(SalePrice ~ TotalSF + LotArea + TotalSF:LotArea, data=mytrain)
yh_hd <- predict(househd, newdata=mytest)
yh_3d <- predict(house3d, newdata=mytest)
ybar <- mean(mytest$SalePrice)
SSTO <- sum( (mytest$SalePrice - ybar)^2 )
SSE_hd <- sum( (mytest$SalePrice - yh_hd)^2 )
SSE_3d <- sum( (mytest$SalePrice - yh_3d)^2 )
rs_hd <- 1 - SSE_hd/SSTO
rs_3d <- 1 - SSE_3d/SSTO
n <- nrow(mytest)
p_3d <- length(house3d)
p_hd <- length(househd)
rsa_hd <- 1 - (n-1)/(n-p_hd)*SSE_hd/SSTO
rsa_3d <- 1 - (n-1)/(n-p_3d)*SSE_3d/SSTO
rsa_hd
summary(househd)
rsa_3d
summary(house3d)
library(tidyverse)
train <- read.csv("C:/Users/Jared/OneDrive/Desktop/Math 425/house-prices-advanced-regression-techniques-1/train.csv", stringsAsFactors = TRUE)
#CORRECTS NA TO BE NONE
train <- train %>%
mutate(Alley = as.character(Alley),
Alley = replace_na(Alley, "None"),
Alley = as.factor(Alley)) %>%
mutate(TotalSF = X1stFlrSF + X2ndFlrSF + TotalBsmtSF,
RichNbrhd = case_when(Neighborhood %in% c("StoneBr", "NridgHt", "NoRidge") ~ 1,
TRUE ~ 0))
View(train)
lm.1stflr <- lm(SalePrice ~ X1stFlrSF, data=train)
summary(lm.1stflr)
plot(SalePrice ~ X1stFlrSF, data=train)
lm.alley <- lm(SalePrice ~ Alley, data=train)
summary(lm.alley)
plot(SalePrice ~ Alley, data=train)
table(train$Alley)
names(train)
#CORRECTS NA TO BE NONE
train <- train %>%
mutate(Alley = as.character(Alley),
Alley = replace_na(Alley, "None"),
Alley = as.factor(Alley)) %>%
#adds total square feet being first second flr and bsmt
#also adds if its a rich neghibor hood or not...
mutate(TotalSF = X1stFlrSF + X2ndFlrSF + TotalBsmtSF,
RichNbrhd = case_when(Neighborhood %in% c("StoneBr", "NridgHt", "NoRidge") ~ 1,
TRUE ~ 0))
```{r}
library(tidyverse)
train <- read.csv("C:/Users/Jared/OneDrive/Desktop/Math 425/house-prices-advanced-regression-techniques-1/train.csv", stringsAsFactors = TRUE)
#CORRECTS NA TO BE NONE
train <- train %>%
mutate(Alley = as.character(Alley),
Alley = replace_na(Alley, "None"),
Alley = as.factor(Alley)) %>%
#adds total square feet being first second flr and bsmt
#also adds if its a rich neghibor hood or not...
mutate(TotalSF = X1stFlrSF + X2ndFlrSF + TotalBsmtSF,
RichNbrhd = case_when(Neighborhood %in% c("StoneBr", "NridgHt", "NoRidge") ~ 1,
TRUE ~ 0))
#sales price predicted by 1st floor sqfeet
lm.1stflr <- lm(SalePrice ~ X1stFlrSF, data=train)
summary(lm.1stflr)
plot(SalePrice ~ X1stFlrSF, data=train)
#price predicted by ally or no ally
lm.alley <- lm(SalePrice ~ Alley, data=train)
summary(lm.alley)
plot(SalePrice ~ Alley, data=train)
table(train$Alley)
lm.fence <- lm(SalePrice ~ Fence, data=train)
summary(lm.fence)
apply(train, 2, function(x) sum(is.na(x))) #count missing values
lm.1stflr <- lm(SalePrice ~ X1stFlrSF, data=train)
summary(lm.1stflr)
lm.2ndflr <- lm(SalePrice ~ X2ndFlrSF, data=train)
summary(lm.2ndflr)
lm.basement <- lm(SalePrice ~ TotalBsmtSF, data=train)
summary(lm.basement)
#Total sqfeet
lm.sqft <- lm(SalePrice ~ TotalSF, data=train)
summary(lm.sqft)
plot(SalePrice ~ TotalSF, data=train)
#lm of price total sqrfeet rich neghibor hood and interaction of the two
lm.sqft.rich <- lm(SalePrice ~ TotalSF + RichNbrhd + TotalSF:RichNbrhd, data=train)
summary(lm.sqft.rich)
lm.sqft.rich.log <- lm(log(SalePrice) ~ TotalSF + RichNbrhd + TotalSF:RichNbrhd, data=train)
summary(lm.sqft.rich)
exp(coef(lm.sqft.rich.log))
househd <- lm(SalePrice ~ TotalSF + LotArea + GarageArea + Alley + FullBath + ScreenPorch, data=train)
summary(househd)
set.seed(121)
num_rows <- 1000 #1460 total
keep <- sample(1:nrow(train), num_rows)
mytrain <- train[keep, ] #Use this in the lm(..., data=mytrain) it is like "rbdata"
mytest <- train[-keep, ] #Use this in the predict(..., newdata=mytest) it is like "rbdata2"
househd <- lm(SalePrice ~ TotalSF + LotArea + GarageArea + Alley + FullBath + ScreenPorch, data=mytrain)
summary(househd)
set.seed(121)
num_rows <- 1000 #1460 total
keep <- sample(1:nrow(train), num_rows)
mytrain <- train[keep, ] #Use this in the lm(..., data=mytrain) it is like "rbdata"
mytest <- train[-keep, ] #Use this in the predict(..., newdata=mytest) it is like "rbdata2"
househd <- lm(SalePrice ~ TotalSF + LotArea + GarageArea + Alley + FullBath + ScreenPorch, data=mytrain)
summary(househd)
house3d <- lm(SalePrice ~ TotalSF + LotArea + TotalSF:LotArea, data=mytrain)
yh_hd <- predict(househd, newdata=mytest)
yh_3d <- predict(house3d, newdata=mytest)
ybar <- mean(mytest$SalePrice)
SSTO <- sum( (mytest$SalePrice - ybar)^2 )
SSE_hd <- sum( (mytest$SalePrice - yh_hd)^2 )
SSE_3d <- sum( (mytest$SalePrice - yh_3d)^2 )
rs_hd <- 1 - SSE_hd/SSTO
rs_3d <- 1 - SSE_3d/SSTO
n <- nrow(mytest)
p_3d <- length(house3d)
p_hd <- length(househd)
rsa_hd <- 1 - (n-1)/(n-p_hd)*SSE_hd/SSTO
rsa_3d <- 1 - (n-1)/(n-p_3d)*SSE_3d/SSTO
rsa_hd
summary(househd)
rsa_3d
summary(house3d)
househd <- lm(SalePrice ~ TotalSF + LotArea + GarageArea + Alley + FullBath + ScreenPorch, data=mytrain)
summary(househd)
yh_hd <- predict(househd, newdata=mytest)
yh_3d <- predict(house3d, newdata=mytest)
ybar <- mean(mytest$SalePrice)
SSTO <- sum( (mytest$SalePrice - ybar)^2 )
SSE_hd <- sum( (mytest$SalePrice - yh_hd)^2 )
SSE_3d <- sum( (mytest$SalePrice - yh_3d)^2 )
rs_hd <- 1 - SSE_hd/SSTO
rs_3d <- 1 - SSE_3d/SSTO
n <- nrow(mytest)
p_3d <- length(house3d)
p_hd <- length(househd)
rsa_hd <- 1 - (n-1)/(n-p_hd)*SSE_hd/SSTO
rsa_3d <- 1 - (n-1)/(n-p_3d)*SSE_3d/SSTO
rsa_hd
summary(househd)
rsa_3d
summary(house3d)
library(tidyverse)
m425 <- read.csv("C:/Users/Jared/OneDrive/Desktop/Math 425/Math425PastGrades.csv", stringsAsFactors=TRUE)
m425 <- m425 %>%
mutate(f70 = ifelse(FinalExam > 70, 1, 0))
View(m425)
pairs(m425, panel=panel.smooth, col=rgb(.2,.2,.2,.2))
glm1 <- glm(f70 ~ Midterm, data=m425, family=binomial)
summary(glm1)
AIC(glm1)
glm1 <- glm(f70 ~ Midterm, data=m425, family=binomial)
summary(glm1)
AIC(glm1)
glm2 <- glm(f70 ~ Midterm + MagicTwoGroups, data=m425, family=binomial)
summary(glm2)
AIC(glm2)
glm3 <- glm(f70 ~ Midterm + MagicTwoGroups + Midterm:MagicTwoGroups, data=m425, family=binomial)
summary(glm3)
AIC(glm3)
glm4 <- glm(f70 ~ Midterm + MagicTwoGroups + Midterm:MagicTwoGroups + , data=m425, family=binomial)
summary(glm4)
glm4 <- glm(f70 ~ Midterm + MagicTwoGroups + Midterm:MagicTwoGroups + , data=m425, family=binomial)
library(tidyverse)
m425 <- read.csv("C:/Users/Jared/OneDrive/Desktop/Math 425/Math425PastGrades.csv", stringsAsFactors=TRUE)
m425 <- m425 %>%
mutate(f70 = ifelse(FinalExam > 70, 1, 0))
pairs(m425, panel=panel.smooth, col=rgb(.2,.2,.2,.2))
glm1 <- glm(f70 ~ Midterm, data=m425, family=binomial)
summary(glm1)
AIC(glm1)
glm2 <- glm(f70 ~ Midterm + MagicTwoGroups, data=m425, family=binomial)
summary(glm2)
AIC(glm2)
glm3 <- glm(f70 ~ Midterm + MagicTwoGroups + Midterm:MagicTwoGroups, data=m425, family=binomial)
summary(glm3)
AIC(glm3)
glm4 <- glm(f70 ~ Midterm + MagicTwoGroups + Midterm:MagicTwoGroups + , data=m425, family=binomial)
m425$ClassActivitiesCompletedPerfectly
glm5 <- glm(f70 ~ Midterm*AssessmentQuizzes*MagicTwoGroups*ClassActivitiesCompletedPerfectly, data=m425, family=binomial)
summary(glm5)
glm5 <- glm(f70 ~ Midterm*MagicTwoGroups + AssessmentQuizzes  + ClassActivitiesCompletedPerfectly,
data=m425, family=binomial)
summary(glm5)
predict(glm5, data.frame(Midterm=70, AssessmentQuizzes=33, ClassActivitiesCompletedPerfectly="Y",MagicTwoGroups=1),
type="response")
summary(m425[,c("Midterm","AssessmentQuizzes","MagicTwoGroups","ClassActivitiesCompletedPerfectly")])
library(ResourceSelection)
install.packages(ResourceSelection)
install.packages(ResourceSelection)
install.packages("ResourceSelection")
hoslem.test(glm5$y, glm5$fit, g=10)
library(ResourceSelection)
hoslem.test(glm5$y, glm5$fit, g=10)
plot(f70 ~ Midterm, data=m425)
b <- coef(glm5)
b
drawit <- function(Magic=1, AQ=33, CA=1){
curve(1/(1+exp(-(b[1] + b[2]*Midterm + b[3]*Magic + b[4]*AQ + b[5]*CA + b[6]*Magic*Midterm))), add=TRUE, xname="Midterm")
}
drawit(Magic=1, AQ=33, CA=1)
points(70, 0.21, col="green", cex=4, pch=16)
keep <- sample(1:nrow(m425), 90)
mytrain <- m425[keep,]
mytest <- m425[-keep,]
glm.test <- glm(f70 ~ Midterm*MagicTwoGroups + AssessmentQuizzes  + ClassActivitiesCompletedPerfectly,
data=mytrain, family=binomial)
mypreds <- predict(glm.test, newdata=mytest, type="response")
mydecs <- ifelse(mypreds > 0.5, 1, 0)
cm <- table(mydecs, mytest$f70)
pcc <- (cm[1] + cm[4]) / sum(cm)
pcc
pairs(m425, panel=panel.smooth, col=rgb(.2,.2,.2,.2))
glm1 <- glm(f70 ~ Midterm, data=m425, family=binomial)
library(tidyverse)
m425 <- read.csv("C:/Users/Jared/OneDrive/Desktop/Math 425/Math425PastGrades.csv", stringsAsFactors=TRUE)
m425 <- m425 %>%
mutate(f70 = ifelse(FinalExam > 70, 1, 0))
pairs(m425, panel=panel.smooth, col=rgb(.2,.2,.2,.2))
glm1 <- glm(f70 ~ Midterm, data=m425, family=binomial)
summary(glm1)
AIC(glm1)
glm2 <- glm(f70 ~ Midterm + MagicTwoGroups, data=m425, family=binomial)
summary(glm2)
AIC(glm2)
glm3 <- glm(f70 ~ Midterm + MagicTwoGroups + Midterm:MagicTwoGroups, data=m425, family=binomial)
summary(glm3)
AIC(glm3)
glm4 <- glm(f70 ~ Midterm + MagicTwoGroups + Midterm:MagicTwoGroups + , data=m425, family=binomial)
