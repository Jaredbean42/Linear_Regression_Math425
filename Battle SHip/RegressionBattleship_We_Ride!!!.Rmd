---
title: "Regression Battleship - Creating your Data"
author: "Jared Bean"
output: 
  html_document:
    theme: cerulean
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r message=FALSE, warning=FALSE}
library(pander)
library(tidyverse)
```

# {.tabset .tabset-pills}

## Instructions 

Using [Desmos](https://www.desmos.com/calculator), design a "true linear regression model" that is **2D-Drawable**, and follows all other **Regression Battleship Rules** (listed below), that is of the form 

$$
  Y_i = \beta_0 + \underbrace{}_\text{Your Model Goes Here} + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0, \sigma^2)
$$ 


Then, use a simulation in R and your linear regression model to obtain a sample of data saved as `rbdata.csv`. 

Your sample of data will be given to other students and your teacher, but this Rmd file (which contains the secret on how you made your data) will remain hidden until after the competition is complete. Your teacher and two of your peers will use the sample of data your provide, `rbdata.csv`, to try to **guess** the **true linear regression model** you used to create the data. The goal is to hide your model well enough that no one can find it, while keeping the R-squared of your data as high as possible.

### Official Rules {.tabset}

#### Advanced Level Competition

Competing in the *Advanced Level* will allow you the opportunity to earn full credit on the Regression Battleship portion of your grade in Math 425 (which is 15% of your Final Grade). However, if you compete at this level, you cannot ever discuss your actual model with your teacher. You can still ask for help from the TA, tutors, or other students that you are not competing against. And you can ask "vague" questions to your teacher as long as it doesn't give too much away about your model.

There are five official rules your model must abide by. If you break any of the rules, you will be disqualified from winning the competition and a grade penalty will result.

1. Your csv file `rbdata.csv` must contain **11 columns of data**.
    * The first column must be your (1) y-variable (labeled as `y`).
    * The other ten columns must be (10) x-variables (labeled as `x1`, `x2`, ... , `x10`). *Please use all lower-case letters.* It does not matter which x-variables you use in your model, and you don't need to use all 10 x-variables in your model.
   
<br/>
    
2. Your **y-variable** (or some transformation of the y-variable) must have been **created from a linear regression model** using only x-variables (or transformations of those x-variables) **from** within **your data set**.
    * Be very careful with transformations. You must ensure that you do not break the rules of a linear regression if you choose to use transformations.
    * If you choose transformations, only these functions are allowed when transforming X and Y variables: `1/Y^2`, `1/Y`, `log(Y)`, `sqrt(Y)`, `sqrt(sqrt(Y))`, `Y^2`, `Y^3`, `1/X^2`, `1/X`, `log(X)`, `sqrt(X)`, `sqrt(sqrt(X))`, `X^2`, `X^3`, `X^4`, and `X^5`. Don't forget to check Rule #3 carefully if you choose transformations.

<br/>
    
3. Your **sample size** must be sufficiently large so that when the true model is fit to your data using lm(...), **all p-values** of terms found in the `summary(...)` output **are significant** at the $\alpha = 0.05$ level.

4. The $R^2$ value ("Multiple R-squared", not the "Adjusted R-squared") of your true model fit on your `rbdata` sample must be greater than or equal to $0.30$. The higher your $R^2$ value, the more impressive your model.

5. Your true model must be **2D-drawable**. This means that it can be drawn in both Desmos and with a single 2D scatterplot in R.

<br/>
<br/>


#### Intermediate Level Competition

Competing in the *Intermediate Level* will **only** allow you to earn **up to 88%** of the full credit that is possible on the Regression Battleship portion of your grade in Math 425 (which is 15% of your Final Grade). *However, getting 88% of the grade is better than failing* the advanced level competition and getting 0% of the Regression Battleship grade. So choose this option if you are not feeling comfortable with your abilities to compete at the Advanced Level. The good news is that if you choose this option, your teacher can help you with your model to make sure everything is correct before you turn it in.

For the Intermediate Level competition, there are also five official rules your model must abide by. If you break any of the rules, you will be disqualified from winning the Intermediate Level competition and a point penalty will be applied to your grade.

1. Your csv file `rbdata.csv` must contain **6 columns of data**.
    * The first column must be your (1) y-variable (labeled as `y`).
    * The other five columns must be (5) x-variables (labeled as `x1`, `x2`, `x3`, `x4` , `x5`). *Please use all lower-case letters.* It does not matter which x-variables you use in your model, and you don't need to use all 5 x-variables in your model.
   
<br/>
    
2. Your **y-variable** must have been **created from a linear regression model** using only x-variables **from** within **your data set**.
    * No transformations of y-variables or x-variables are allowed in the Intermediate Level competition.

<br/>
    
3. Your **sample size** must be sufficiently large so that when the true model is fit to your data using lm(...), **all p-values** of terms found in the `summary(...)` output **are significant** at the $\alpha = 0.05$ level.

4. The $R^2$ value ("Multiple R-squared", not the "Adjusted R-squared") of your true model fit on your `rbdata` sample must be greater than or equal to $0.80$.

5. Your true model must be **2D-drawable**. This means that it can be drawn in Desmos and with a single 2D scatterplot in R.

<br/>
<br/>



## Desmos 

Start by creating a picture of your true model in Desmos. Snip a screenshot of your completed model. Include a picture of your Desmos graph showing your true model.

(update to new path...)




## Code

Use the R-chunks below to create your simulated sample of data from your true regression model.


```{r message=FALSE, warning=FALSE, comment=NA}
set.seed(42) #This ensures the randomness is the "same" everytime if you play the entire R-chunk as one entire piece of code. If you run lines separately, your data might not come out the same every time. You can pick any integer value you want for set.seed. Each choice produces a different sample, so you might want to play around with a few different choices.

## To begin, decide on your sample size. (You may have to revise it later to ensure all values in your lm(...) are significant.)
  
 n <- 500
  
## Then, create 10 X-variables using functions like rnorm(n, mean, sd), rchisq(n, df), rf(n, df1, df2), rt(n, df), rbeta(n, a, b), runif(n, a, b) or sample(c(1,0), n, replace=TRUE)... ## To see what any of these functions do, run codes like hist(rchisq(n, 3)). These functions are simply allowing you to get a random sample of x-values. But the way you choose your x-values can have quite an impact on what the final scatterplot of the data will look like.

 
 x2 <- runif(n, -8, 8) # TRUE x axis
 

 
 x5 <- sample(c(0,1), n, replace=TRUE) #true switch for red parabola
 x3 <- sample(c(0,1), n, replace=TRUE) #true switch for green line
 #x3[x5==1] <- 0 #This makes it so they can't both be on
 
 
 
 x1 <- runif(n, -8, 8) #replace this
 x4 <- runif(n, -8, 8) #experamental switch pt1
 x6 <- runif(n, -8, 8) #exp switch pt2
 x7 <- runif(n, -8, 8) #replace this
 x8 <- runif(n, -8, 8) #replace this
 x9 <- runif(n, -8, 8) #replace this
 x10 <- runif(n, -8, 8) #replace this
 

 
 z1 <- 0 #true switch that is impossible to find now
 z1[x4>3 & x6<3] <- 1
 
 x2 <- ( x9 / x10)
 # GOLDEN??? What is up with adding in switches. also adding doesnt do anything 
#Too much sauce ruins the lm so be carful. multiply and divide
 #Not quite do something like this
 
 #True x axis It now exisits outside the model
 z0 <- (x9+x10+x8)
 
 
## Then, create betas, sigma, normal error terms and y
 
 
 #Primary quad
 beta0 <- -2
 beta1 <- 0.001
 beta3 <- 0.01
 
 #Changes needed for next quadratic
 beta4 <- 4
 beta5 <- -0.001
 beta6 <- -0.02
 
 #changes needed for a green desc line
 beta7 <- 2
 beta8 <- -0.201
 beta9 <- -0.01

 #changes needed for positive orange line
 beta10 <- -4
 beta11 <- 0.401
 beta12 <- 0.02
   
   
 sigma <- 3 #change to whatever positive number you want
 

 ################################
 # You ARE NOT ALLOWED to change this part:
 epsilon_i <- rnorm(n, 0, sigma)
 ################################ 
 
 #An example of how to make Y...
 # y <-  beta0 + beta1*X1 + beta2*X2 + beta3*X4*X2 + epsilon_i
 
 
 y <- beta0 + beta1*x2 + beta3*x2^2 +  #primary quadratic blue
      (beta4 + beta5*x2 + beta6*x2^2)*x5 + #second quadratic red (x5 is switch)
      (beta7 + beta8*x2 + beta9*x2^2)*x3 + #decreasing line green
      (beta10 + beta11*x2 + beta12*x2^2)*x3*x5 #increasing orange line

 + epsilon_i
 #...edit this code and replace it with your model. Don't forget the + epsilon_i!
 
 
 ## Now, you need to load your x-variables and y-variable 
 ## into a data set.
 # You can include Y' or X' instead of Y or X if you wish.
 # Remember, only these functions are allowed when transforming
 # variables: 1/Y^2, 1/Y, log(Y), sqrt(Y), sqrt(sqrt(Y)), Y^2, Y^3, 1/X^2, 1/X, log(X), sqrt(X), sqrt(sqrt(X)), X^2, X^3, X^4, X^5. 
 #########################################################
 # ILLEGAL: Y = (beta0 + beta1*X5)^2 + epsilon_i #########
 #########################################################
 # Legal: sqrt(Y) = beta0 + beta1*X5^2 + epsilon_i #######
 #########################################################
 # You can only transform individual terms, not groups of terms.
 # And the beta's cannot be part of the x-transformations.

 # This loads your data into a data set:
 rbdata <- data.frame(y, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10)
 
 

 
 #Now fit your model to make sure it comes out significant:
 #Fixed?
 mylm <- lm(y ~ x2 + I(x2^2) +  #primary quadratic blue
      (x2 + I(x2^2))*x5 + #second quadratic red (x5 is switch)
      (x2 + I(x2^2))*x3 + #decreasing line green
      (x2 + I(x2^2))*x3*x5) #orange line

 
 #edit this code to be your true model
 summary(mylm)
 #all p-values must be significant
 #the R^2 value must be greater than or equal to 0.30.
 

# Once you are done with creating your model, and have successfully
# graphed it (see below), un-comment the following `write.csv` code,
# then, PLAY this ENTIRE R-chunk to write your data to a csv.

 write.csv(rbdata, "rbdata.csv", row.names=FALSE)

# The above code writes the dataset to your "current directory"
# To see where that is, use: getwd() in your Console.
# Find the rbdata.csv data set and upload it to I-Learn.
```






## R Plot

Provide a 2D scatterplot that shows both your *true* model (dashed lines) and *estimated* model (solid lines) on the same scatterplot. This should match your Desmos graph. 

```{r}

palette(c("steelblue","red", "green", "orange"))
plot(y~x2, 
     
     data=rbdata,
     xlim=c(-8,8),
     ylim=c(-4,4),
     col=interaction(x5, x3)
     )

#Make true lines 

x5=0
x3=0
curve(beta0 + beta1*x2 + beta3*x2^2 +  #primary quadratic blue
      (beta4 + beta5*x2 + beta6*x2^2)*x5 + #second quadratic red (x5 is switch)
      (beta7 + beta8*x2 + beta9*x2^2)*x3 + #decreasing line green
      (beta10 + beta11*x2 + beta12*x2^2)*x3*x5 #orange line
      , add=TRUE, xname="x2", col="steelblue", lwd=5, lty=3)
x5=1
x3=0
curve(beta0 + beta1*x2 + beta3*x2^2 +  #primary quadratic blue
      (beta4 + beta5*x2 + beta6*x2^2)*x5 + #second quadratic red (x5 is switch)
      (beta7 + beta8*x2 + beta9*x2^2)*x3 + #decreasing line green
      (beta10 + beta11*x2 + beta12*x2^2)*x3*x5 #orange line
      , add=TRUE, xname="x2", col="red", lwd=5, lty=3)
x5=0
x3=1
curve(beta0 + beta1*x2 + beta3*x2^2 +  #primary quadratic blue
      (beta4 + beta5*x2 + beta6*x2^2)*x5 + #second quadratic red (x5 is switch)
      (beta7 + beta8*x2 + beta9*x2^2)*x3 + #decreasing line green
      (beta10 + beta11*x2 + beta12*x2^2)*x3*x5 #orange line
      , add=TRUE, xname="x2", col="green", lwd=5, lty=3)
x5=1
x3=1
curve(beta0 + beta1*x2 + beta3*x2^2 +  #primary quadratic blue
      (beta4 + beta5*x2 + beta6*x2^2)*x5 + #second quadratic red (x5 is switch)
      (beta7 + beta8*x2 + beta9*x2^2)*x3 + #decreasing line green
      (beta10 + beta11*x2 + beta12*x2^2)*x3*x5 #orange line
      , add=TRUE, xname="x2", col="orange", lwd=5, lty=3)



#To make lm curves match the b on the regression
b <- coef(mylm)

#This is predicted curves
x5=0
x3=0
curve(b[1] + b[2]*x2 + b[3]*x2^2 + b[4]*x5 + b[5]*x3 + b[6]*x2*x5 + 
      b[7]*x2^2*x5 + b[8]*x2*x3 + b[9]*x2^2*x3 + b[10]*x5*x3 + b[11]*x2*x5*x3 +
      b[12]*x2^2*x5*x3, 
      add=TRUE, xname="x2", col="black", lwd=1)

x5=1
x3=0
curve(b[1] + b[2]*x2 + b[3]*x2^2 + b[4]*x5 + b[5]*x3 + b[6]*x2*x5 + 
      b[7]*x2^2*x5 + b[8]*x2*x3 + b[9]*x2^2*x3 + b[10]*x5*x3 + b[11]*x2*x5*x3 +
      b[12]*x2^2*x5*x3, 
      add=TRUE, xname="x2", col="black", lwd=1)
x5=0
x3=1
curve(b[1] + b[2]*x2 + b[3]*x2^2 + b[4]*x5 + b[5]*x3 + b[6]*x2*x5 + 
      b[7]*x2^2*x5 + b[8]*x2*x3 + b[9]*x2^2*x3 + b[10]*x5*x3 + b[11]*x2*x5*x3 +
      b[12]*x2^2*x5*x3, 
      add=TRUE, xname="x2", col="black", lwd=1)
x5=1
x3=1
curve(b[1] + b[2]*x2 + b[3]*x2^2 + b[4]*x5 + b[5]*x3 + b[6]*x2*x5 + 
      b[7]*x2^2*x5 + b[8]*x2*x3 + b[9]*x2^2*x3 + b[10]*x5*x3 + b[11]*x2*x5*x3 +
      b[12]*x2^2*x5*x3, 
      add=TRUE, xname="x2", col="black", lwd=1)


#add legend
legend("bottomright", 
       legend = c("mylm lines"), 
       col = "black", 
       lwd = 2)
```


## Math Model

Write out your "true" model in mathematical form. Make sure it matches your code. This could be "painful" if you chose a complicated model.

$$
Y_i = \beta_0 + \underbrace{\beta_1 x_2 + \beta_3 x_2^2}_{\text{Primary quadratic (blue)}} + \underbrace{(\beta_4 + \beta_5 x_2 + \beta_6 x_2^2) x_5}_{\text{Second quadratic (red, switch } x_5 \text{)}} + \underbrace{(\beta_7 + \beta_8 x_2 + \beta_9 x_2^2) x_3}_{\text{Decreasing line (green)}} + \underbrace{(\beta_{10} + \beta_{11} x_2 + \beta_{12} x_2^2) x_3 x_5}_{\text{Orange Line}} + \epsilon_i \quad \text{where} \ \epsilon_i \sim N(0, \sigma^2)
$$


## Results

Once the Regression Battleship competition is completed, you will be given instructions on how to complete this section. The basic idea is to compare the three guesses at your true model (from two peers, and your teacher) to decide who won (i.e., who had the closest guess).

```{r echo=FALSE, message=FALSE, warning=FALSE, results="hide", include=FALSE}
set.seed(21)#This ensures the randomness is the "same" everytime if you play the entire R-chunk as one entire piece of code. If you run lines separately, your data might not come out the same every time. You can pick any integer value you want for set.seed. Each choice produces a different sample, so you might want to play around with a few different choices.

## To begin, decide on your sample size. (You may have to revise it later to ensure all values in your lm(...) are significant.)
  
 n <- 500
  
## Then, create 10 X-variables using functions like rnorm(n, mean, sd), rchisq(n, df), rf(n, df1, df2), rt(n, df), rbeta(n, a, b), runif(n, a, b) or sample(c(1,0), n, replace=TRUE)... ## To see what any of these functions do, run codes like hist(rchisq(n, 3)). These functions are simply allowing you to get a random sample of x-values. But the way you choose your x-values can have quite an impact on what the final scatterplot of the data will look like.

 
 x2 <- runif(n, -8, 8) # TRUE x axis
 

 
 x5 <- sample(c(0,1), n, replace=TRUE) #true switch for red parabola
 x3 <- sample(c(0,1), n, replace=TRUE) #true switch for green line
 #x3[x5==1] <- 0 #This makes it so they can't both be on
 
 
 
 x1 <- runif(n, -8, 8) #replace this
 x4 <- runif(n, -8, 8) #experamental switch pt1
 x6 <- runif(n, -8, 8) #exp switch pt2
 x7 <- runif(n, -8, 8) #replace this
 x8 <- runif(n, -8, 8) #replace this
 x9 <- runif(n, -8, 8) #replace this
 x10 <- runif(n, -8, 8) #replace this
 

 
 z1 <- 0 #true switch that is impossible to find now
 z1[x4>3 & x6<3] <- 0
 
 x2 <- ( x9 / x10)
 # GOLDEN??? What is up with adding in switches. also adding doesnt do anything 
#Too much sauce ruins the lm so be carful. multiply and divide
 #Not quite do something like this
 
 #True x axis It now exisits outside the model
 z0 <- (x9+x10+x8)
 
 
## Then, create betas, sigma, normal error terms and y
 
 
 #Primary quad
 beta0 <- -2
 beta1 <- 0.001
 beta3 <- 0.01
 
 #Changes needed for next quadratic
 beta4 <- 4
 beta5 <- -0.001
 beta6 <- -0.02
 
 #changes needed for a green desc line
 beta7 <- 2
 beta8 <- -0.201
 beta9 <- -0.01

 #changes needed for positive orange line
 beta10 <- -4
 beta11 <- 0.401
 beta12 <- 0.02
   
   
 sigma <- 3 #change to whatever positive number you want
 

 ################################
 # You ARE NOT ALLOWED to change this part:
 epsilon_i <- rnorm(n, 0, sigma)
 ################################ 
 
 #An example of how to make Y...
 # y <-  beta0 + beta1*X1 + beta2*X2 + beta3*X4*X2 + epsilon_i
 
 
 y <- beta0 + beta1*x2 + beta3*x2^2 +  #primary quadratic blue
      (beta4 + beta5*x2 + beta6*x2^2)*x5 + #second quadratic red (x5 is switch)
      (beta7 + beta8*x2 + beta9*x2^2)*x3 + #decreasing line green
      (beta10 + beta11*x2 + beta12*x2^2)*x3*x5 #increasing orange line

 + epsilon_i
 #...edit this code and replace it with your model. Don't forget the + epsilon_i!
 
 
 ## Now, you need to load your x-variables and y-variable 
 ## into a data set.
 # You can include Y' or X' instead of Y or X if you wish.
 # Remember, only these functions are allowed when transforming
 # variables: 1/Y^2, 1/Y, log(Y), sqrt(Y), sqrt(sqrt(Y)), Y^2, Y^3, 1/X^2, 1/X, log(X), sqrt(X), sqrt(sqrt(X)), X^2, X^3, X^4, X^5. 
 #########################################################
 # ILLEGAL: Y = (beta0 + beta1*X5)^2 + epsilon_i #########
 #########################################################
 # Legal: sqrt(Y) = beta0 + beta1*X5^2 + epsilon_i #######
 #########################################################
 # You can only transform individual terms, not groups of terms.
 # And the beta's cannot be part of the x-transformations.

 # This loads your data into a data set:
 thedata2 <- data.frame(y, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10)
 
 

```



# Validation table
```{r message=FALSE, warning=FALSE, paged.print=FALSE}
#Validation table

#old lm training
 lmt <- lm(y ~ x2 + I(x2^2) +  #primary quadratic blue
      (x2 + I(x2^2))*x5 + #second quadratic red (x5 is switch)
      (x2 + I(x2^2))*x3 + #decreasing line green
      (x2 + I(x2^2))*x3*x5, data=rbdata) #orange line

 lms <- lm(y ~ x2 + I(x2^2) + I(x2^3) + x5 + x5:x2 + x5:I(x2^2) + 
    x3 + x3:x2 + x3:I(x2^2) + x3:x5 + x3:x5:x2 + x3:x5:I(x2^2), data=rbdata)

 lmc <- lm(y ~ x2 + I(x2^2) + x3 + x2:x3 + x5 + I(x2^2):x3 + I(x2^2):x5 + 
             x3:x5 + I(x2^2):x3:x5 + x2:x3:x5, data=rbdata)

#New data
  yht <- predict(lmt, newdata=thedata2)
  yhs <- predict(lms, newdata=thedata2)
  yhc <- predict(lmc, newdata=thedata2)  
  
  
  
  
# Compute y-bar
ybar <- mean(thedata2$y) #Yi is given by Ynew from the new sample of data
  
# Compute SSTO
SSTO <- sum( (thedata2$y - ybar)^2 )
  
# Compute SSE for each model using y - yhat
SSEt <- sum( (thedata2$y - yht)^2 )
SSEs <- sum( (thedata2$y - yhs)^2 )
SSEc <- sum( (thedata2$y - yhc)^2 )
  
# Compute R-squared for each
rst <- 1 - SSEt/SSTO
rss <- 1 - SSEs/SSTO
rsc <- 1 - SSEc/SSTO
  
# Compute adjusted R-squared for each
n <- length(thedata2$y) #sample size
pt <- length(coef(lmt)) #num. parameters in model
ps <- length(coef(lms)) #num. parameters in model
pc <- length(coef(lmc)) #num. parameters in model
rsta <- 1 - (n-1)/(n-pt)*SSEt/SSTO
rssa <- 1 - (n-1)/(n-ps)*SSEs/SSTO
rsca <- 1 - (n-1)/(n-pc)*SSEc/SSTO
  

my_output_table2 <- data.frame(Model = c("True", "Simple", "Complicated"), `Original R2` = c(summary(lmt)$r.squared, summary(lms)$r.squared, summary(lmc)$r.squared), `Orig. Adj. R-squared` = c(summary(lmt)$adj.r.squared, summary(lms)$adj.r.squared, summary(lmc)$adj.r.squared), `Validation R-squared` = c(rst, rss, rsc), `Validation Adj. R^2` = c(rsta, rssa, rsca))


colnames(my_output_table2) <- c("Model", "Original $R^2$", "Original Adj. $R^2$", "Validation $R^2$", "Validation Adj. $R^2$")

knitr::kable(my_output_table2, escape=TRUE, digits=4)
   
```


# Graph
```{r}

plot(y ~ x2, data=thedata2, col=interaction(x5,x3), xlim=c(-8,8), ylim=c(-4,4))
points(lms$fit ~ x2, data=rbdata, col=interaction(x5,x3), pch=16, cex=0.5)

b <- coef(lms)

#Saunders guess
drawit <- function(x5=0, x3=0, i=1){
  curve(b[1] + b[2]*x2 + b[3]*x2^2 + b[4]*x2^3 + b[5]*x5 + b[6]*x3 + b[7]*x2*x5 + b[8]*x2^2*x5 + b[9]*x2*x3 + b[10]*x2^2*x3 + b[11]*x5*x3 + b[12]*x2*x5*x3 + b[13]*x2^2*x5*x3, add=TRUE, xname="x2", col=palette()[i])  
}

drawit(0,0,1)
drawit(1,0,2)
drawit(0,1,3)
drawit(1,1,4)


#True model
x5=0
x3=0
curve(beta0 + beta1*x2 + beta3*x2^2 +  #primary quadratic blue
      (beta4 + beta5*x2 + beta6*x2^2)*x5 + #second quadratic red (x5 is switch)
      (beta7 + beta8*x2 + beta9*x2^2)*x3 + #decreasing line green
      (beta10 + beta11*x2 + beta12*x2^2)*x3*x5 #orange line
      , add=TRUE, xname="x2", col="black", lwd=3, lty=2)
x5=1
x3=0
curve(beta0 + beta1*x2 + beta3*x2^2 +  #primary quadratic blue
      (beta4 + beta5*x2 + beta6*x2^2)*x5 + #second quadratic red (x5 is switch)
      (beta7 + beta8*x2 + beta9*x2^2)*x3 + #decreasing line green
      (beta10 + beta11*x2 + beta12*x2^2)*x3*x5 #orange line
      , add=TRUE, xname="x2", col="black", lwd=3, lty=2)
x5=0
x3=1
curve(beta0 + beta1*x2 + beta3*x2^2 +  #primary quadratic blue
      (beta4 + beta5*x2 + beta6*x2^2)*x5 + #second quadratic red (x5 is switch)
      (beta7 + beta8*x2 + beta9*x2^2)*x3 + #decreasing line green
      (beta10 + beta11*x2 + beta12*x2^2)*x3*x5 #orange line
      , add=TRUE, xname="x2", col="black", lwd=3, lty=2)
x5=1
x3=1
curve(beta0 + beta1*x2 + beta3*x2^2 +  #primary quadratic blue
      (beta4 + beta5*x2 + beta6*x2^2)*x5 + #second quadratic red (x5 is switch)
      (beta7 + beta8*x2 + beta9*x2^2)*x3 + #decreasing line green
      (beta10 + beta11*x2 + beta12*x2^2)*x3*x5 #orange line
      , add=TRUE, xname="x2", col="black", lwd=3, lty=2)


```


Both my peers and Brother Saunders found my model. 

```{r message=FALSE, warning=FALSE}
#The table below compares the estimated coefficients from two models to the true #parameters.


library(knitr)

# Creating the comparison table
comparison_table <- data.frame(
  Parameter = c("β₀ (Intercept)", "β₁ (x2)", "β₃ (I(x2²))", "β₄ (x5)", "β₅ (x2:x5)", 
                "β₆ (I(x2²):x5)", "β₇ (x3)", "β₈ (x2:x3)", "β₉ (I(x2²):x3)", 
                "β₁₀ (x3:x5)", "β₁₁ (x2:x3:x5)", "β₁₂ (I(x2²):x3:x5)"),
  True_Value = c(-2, 0.001, 0.01, 4, -0.001, -0.02, 2, -0.201, -0.01, -4, 0.401, 0.02),
  First_Model_Range = c("(-2.0004, -1.9993)", "(0.0007, 0.0008)", "(0.00999, 0.009999)", 
                        "(3.9991, 4.0006)", "N/A", "(-0.0200047, -0.0200037)", 
                        "(1.9991, 2.0006)", "(-0.2008, -0.2006)", "(-0.0099999, -0.0099981)", 
                        "(-4.0009, -3.9988)", "(0.3999, 0.4001)", "(0.0200, 0.02001)"),
  Second_Model_CI = c("(-2, -2)", "(0.001, 0.001)", "(0.01, 0.01)", "(4, 4)", 
                      "(-0.001, -0.001)", "(-0.02, -0.02)", "(2, 2)", 
                      "(-0.201, -0.201)", "(-0.01, -0.01)", "(-4, -4)", "(0.401, 0.401)", "(0.02, 0.02)"),
  Captured = c("✅ Yes", "✅ Close but slightly low", "✅ Yes", "✅ Yes", "✅ Only in second model",
               "✅ Yes", "✅ Yes", "✅ Yes", "✅ Yes", "✅ Yes", "✅ First model slightly low, second is exact", "✅ Yes")
)

# Print the table using kable
kable(comparison_table, format = "markdown", align = "c", col.names = c("Parameter", "True Value", "First Model (Range)", "Second Model (CI 2.5%-97.5%)", "Captured?"))
```


# Final Verdict: 

The second model fully captures the true parameters, while the first model is slightly off due to missing x2:x5. This was close but I have to give the win to my classmates. This has been my favorite homework assignment ever. Thanks Brother Saunders.   



 

 